# Chinese-CLIP 微调策略说明

本文档详细介绍了Chinese-CLIP模型的微调策略和方法。

## 微调策略概述

本项目采用了三阶段渐进式微调策略，以获得最佳的模型性能：

1. **大规模数据集初步微调**（MUGE数据集）
2. **中等规模数据集进一步微调**（Flickr30k-CN数据集）
3. **小规模私有数据集精细微调**（自定义领域数据集）

这种渐进式微调策略的优势在于能够保留预训练模型的通用性，同时逐步适应特定领域的需求。

## 微调参数设置

### MUGE数据集微调

- **批量大小**：128
- **学习率**：1e-5
- **训练轮次**：5
- **权重衰减**：0.2
- **优化器**：AdamW

### Flickr30k-CN数据集微调

- **批量大小**：64
- **学习率**：5e-6
- **训练轮次**：10
- **权重衰减**：0.2
- **优化器**：AdamW

### 私有数据集微调

- **批量大小**：16
- **学习率**：1e-6
- **训练轮次**：30
- **权重衰减**：0.1
- **优化器**：AdamW

## 数据增强技术

在微调过程中，我们使用了以下数据增强技术来提高模型的泛化能力：

- **随机裁剪**：增加图像变化
- **随机水平翻转**：增加图像变化
- **颜色抖动**：微调亮度、对比度、饱和度

## 模型选择策略

在每个阶段结束后，我们采用以下指标来选择最佳模型：

1. **验证集准确率**：主要评估指标
2. **文本-图像检索性能**（Recall@1, Recall@5, Recall@10）
3. **图像-文本检索性能**（Recall@1, Recall@5, Recall@10）

## 微调技巧

1. **学习率热身**：在训练初期使用较小的学习率，逐步增加到目标学习率
2. **分层学习率**：为不同层设置不同的学习率，通常深层使用较小的学习率
3. **提前停止**：当验证集性能连续多轮没有提升时停止训练
4. **梯度裁剪**：防止梯度爆炸
5. **混合精度训练**：使用FP16加速训练过程

## 实验结果

多轮实验表明，使用上述三阶段微调策略，最终模型在以下方面有明显提升：

- 中文图文匹配准确率提高10-15%
- 特定领域查询的相关性提高20%以上
- 推理速度保持基本不变

## 建议与最佳实践

1. **数据质量**：确保私有数据集的标注质量和图文一致性
2. **超参数调优**：根据具体任务和数据集大小调整学习率和批量大小
3. **模型选择**：如果GPU内存有限，可以考虑使用较小的ViT-B/16模型
4. **训练监控**：始终监控训练和验证损失，避免过拟合 